{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b189346f-9150-4544-8fd3-223f204a6d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82720b86-4bf6-4f21-a3db-f68c69951148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86d3be72-304f-4ebc-bb10-1c99444d1de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b453c2fe-6b05-4868-b66e-3834613a490e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True,transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True,transform=transform)\n",
    "\n",
    "#train_dataset=torch.utils.data.Subset(train_dataset,range(100))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1db6f04-1c8a-41ea-af30-db7b96880b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d945c94-a753-4e58-a2c4-7ac0ff85fd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03a6d953-4313-443b-90ac-3d6e9c127d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6, Batch 0/600, Loss: 2.3263\n",
      "Epoch 1/6, Batch 10/600, Loss: 1.0349\n",
      "Epoch 1/6, Batch 20/600, Loss: 0.5995\n",
      "Epoch 1/6, Batch 30/600, Loss: 0.4544\n",
      "Epoch 1/6, Batch 40/600, Loss: 0.4364\n",
      "Epoch 1/6, Batch 50/600, Loss: 0.3829\n",
      "Epoch 1/6, Batch 60/600, Loss: 0.3964\n",
      "Epoch 1/6, Batch 70/600, Loss: 0.4007\n",
      "Epoch 1/6, Batch 80/600, Loss: 0.4105\n",
      "Epoch 1/6, Batch 90/600, Loss: 0.5332\n",
      "Epoch 1/6, Batch 100/600, Loss: 0.5078\n",
      "Epoch 1/6, Batch 110/600, Loss: 0.4148\n",
      "Epoch 1/6, Batch 120/600, Loss: 0.2961\n",
      "Epoch 1/6, Batch 130/600, Loss: 0.2452\n",
      "Epoch 1/6, Batch 140/600, Loss: 0.3669\n",
      "Epoch 1/6, Batch 150/600, Loss: 0.2225\n",
      "Epoch 1/6, Batch 160/600, Loss: 0.2859\n",
      "Epoch 1/6, Batch 170/600, Loss: 0.3219\n",
      "Epoch 1/6, Batch 180/600, Loss: 0.1997\n",
      "Epoch 1/6, Batch 190/600, Loss: 0.2455\n",
      "Epoch 1/6, Batch 200/600, Loss: 0.1897\n",
      "Epoch 1/6, Batch 210/600, Loss: 0.1384\n",
      "Epoch 1/6, Batch 220/600, Loss: 0.3589\n",
      "Epoch 1/6, Batch 230/600, Loss: 0.3782\n",
      "Epoch 1/6, Batch 240/600, Loss: 0.2096\n",
      "Epoch 1/6, Batch 250/600, Loss: 0.2526\n",
      "Epoch 1/6, Batch 260/600, Loss: 0.1685\n",
      "Epoch 1/6, Batch 270/600, Loss: 0.3823\n",
      "Epoch 1/6, Batch 280/600, Loss: 0.2077\n",
      "Epoch 1/6, Batch 290/600, Loss: 0.2126\n",
      "Epoch 1/6, Batch 300/600, Loss: 0.2827\n",
      "Epoch 1/6, Batch 310/600, Loss: 0.1390\n",
      "Epoch 1/6, Batch 320/600, Loss: 0.2664\n",
      "Epoch 1/6, Batch 330/600, Loss: 0.1701\n",
      "Epoch 1/6, Batch 340/600, Loss: 0.3319\n",
      "Epoch 1/6, Batch 350/600, Loss: 0.1801\n",
      "Epoch 1/6, Batch 360/600, Loss: 0.2946\n",
      "Epoch 1/6, Batch 370/600, Loss: 0.1770\n",
      "Epoch 1/6, Batch 380/600, Loss: 0.2661\n",
      "Epoch 1/6, Batch 390/600, Loss: 0.1400\n",
      "Epoch 1/6, Batch 400/600, Loss: 0.2040\n",
      "Epoch 1/6, Batch 410/600, Loss: 0.1586\n",
      "Epoch 1/6, Batch 420/600, Loss: 0.2853\n",
      "Epoch 1/6, Batch 430/600, Loss: 0.1463\n",
      "Epoch 1/6, Batch 440/600, Loss: 0.2755\n",
      "Epoch 1/6, Batch 450/600, Loss: 0.2938\n",
      "Epoch 1/6, Batch 460/600, Loss: 0.1468\n",
      "Epoch 1/6, Batch 470/600, Loss: 0.3153\n",
      "Epoch 1/6, Batch 480/600, Loss: 0.2798\n",
      "Epoch 1/6, Batch 490/600, Loss: 0.1331\n",
      "Epoch 1/6, Batch 500/600, Loss: 0.1776\n",
      "Epoch 1/6, Batch 510/600, Loss: 0.1046\n",
      "Epoch 1/6, Batch 520/600, Loss: 0.2236\n",
      "Epoch 1/6, Batch 530/600, Loss: 0.1709\n",
      "Epoch 1/6, Batch 540/600, Loss: 0.2257\n",
      "Epoch 1/6, Batch 550/600, Loss: 0.1072\n",
      "Epoch 1/6, Batch 560/600, Loss: 0.0719\n",
      "Epoch 1/6, Batch 570/600, Loss: 0.1492\n",
      "Epoch 1/6, Batch 580/600, Loss: 0.3019\n",
      "Epoch 1/6, Batch 590/600, Loss: 0.1201\n",
      "Epoch 2/6, Batch 0/600, Loss: 0.1929\n",
      "Epoch 2/6, Batch 10/600, Loss: 0.0462\n",
      "Epoch 2/6, Batch 20/600, Loss: 0.0695\n",
      "Epoch 2/6, Batch 30/600, Loss: 0.1442\n",
      "Epoch 2/6, Batch 40/600, Loss: 0.1026\n",
      "Epoch 2/6, Batch 50/600, Loss: 0.1822\n",
      "Epoch 2/6, Batch 60/600, Loss: 0.2205\n",
      "Epoch 2/6, Batch 70/600, Loss: 0.2050\n",
      "Epoch 2/6, Batch 80/600, Loss: 0.1577\n",
      "Epoch 2/6, Batch 90/600, Loss: 0.1553\n",
      "Epoch 2/6, Batch 100/600, Loss: 0.1263\n",
      "Epoch 2/6, Batch 110/600, Loss: 0.2708\n",
      "Epoch 2/6, Batch 120/600, Loss: 0.1208\n",
      "Epoch 2/6, Batch 130/600, Loss: 0.0827\n",
      "Epoch 2/6, Batch 140/600, Loss: 0.1303\n",
      "Epoch 2/6, Batch 150/600, Loss: 0.0319\n",
      "Epoch 2/6, Batch 160/600, Loss: 0.2989\n",
      "Epoch 2/6, Batch 170/600, Loss: 0.1323\n",
      "Epoch 2/6, Batch 180/600, Loss: 0.0725\n",
      "Epoch 2/6, Batch 190/600, Loss: 0.1187\n",
      "Epoch 2/6, Batch 200/600, Loss: 0.1357\n",
      "Epoch 2/6, Batch 210/600, Loss: 0.0991\n",
      "Epoch 2/6, Batch 220/600, Loss: 0.0493\n",
      "Epoch 2/6, Batch 230/600, Loss: 0.1597\n",
      "Epoch 2/6, Batch 240/600, Loss: 0.1182\n",
      "Epoch 2/6, Batch 250/600, Loss: 0.0829\n",
      "Epoch 2/6, Batch 260/600, Loss: 0.1152\n",
      "Epoch 2/6, Batch 270/600, Loss: 0.1344\n",
      "Epoch 2/6, Batch 280/600, Loss: 0.1933\n",
      "Epoch 2/6, Batch 290/600, Loss: 0.1204\n",
      "Epoch 2/6, Batch 300/600, Loss: 0.1305\n",
      "Epoch 2/6, Batch 310/600, Loss: 0.2140\n",
      "Epoch 2/6, Batch 320/600, Loss: 0.1783\n",
      "Epoch 2/6, Batch 330/600, Loss: 0.0682\n",
      "Epoch 2/6, Batch 340/600, Loss: 0.3126\n",
      "Epoch 2/6, Batch 350/600, Loss: 0.1889\n",
      "Epoch 2/6, Batch 360/600, Loss: 0.1400\n",
      "Epoch 2/6, Batch 370/600, Loss: 0.0743\n",
      "Epoch 2/6, Batch 380/600, Loss: 0.1169\n",
      "Epoch 2/6, Batch 390/600, Loss: 0.0360\n",
      "Epoch 2/6, Batch 400/600, Loss: 0.1955\n",
      "Epoch 2/6, Batch 410/600, Loss: 0.1979\n",
      "Epoch 2/6, Batch 420/600, Loss: 0.0727\n",
      "Epoch 2/6, Batch 430/600, Loss: 0.0655\n",
      "Epoch 2/6, Batch 440/600, Loss: 0.1306\n",
      "Epoch 2/6, Batch 450/600, Loss: 0.0890\n",
      "Epoch 2/6, Batch 460/600, Loss: 0.1136\n",
      "Epoch 2/6, Batch 470/600, Loss: 0.0723\n",
      "Epoch 2/6, Batch 480/600, Loss: 0.1194\n",
      "Epoch 2/6, Batch 490/600, Loss: 0.0915\n",
      "Epoch 2/6, Batch 500/600, Loss: 0.1232\n",
      "Epoch 2/6, Batch 510/600, Loss: 0.1240\n",
      "Epoch 2/6, Batch 520/600, Loss: 0.1209\n",
      "Epoch 2/6, Batch 530/600, Loss: 0.1065\n",
      "Epoch 2/6, Batch 540/600, Loss: 0.0615\n",
      "Epoch 2/6, Batch 550/600, Loss: 0.1300\n",
      "Epoch 2/6, Batch 560/600, Loss: 0.1151\n",
      "Epoch 2/6, Batch 570/600, Loss: 0.2494\n",
      "Epoch 2/6, Batch 580/600, Loss: 0.1206\n",
      "Epoch 2/6, Batch 590/600, Loss: 0.1051\n",
      "Epoch 3/6, Batch 0/600, Loss: 0.0603\n",
      "Epoch 3/6, Batch 10/600, Loss: 0.0948\n",
      "Epoch 3/6, Batch 20/600, Loss: 0.0788\n",
      "Epoch 3/6, Batch 30/600, Loss: 0.1596\n",
      "Epoch 3/6, Batch 40/600, Loss: 0.0456\n",
      "Epoch 3/6, Batch 50/600, Loss: 0.1163\n",
      "Epoch 3/6, Batch 60/600, Loss: 0.1090\n",
      "Epoch 3/6, Batch 70/600, Loss: 0.1652\n",
      "Epoch 3/6, Batch 80/600, Loss: 0.1424\n",
      "Epoch 3/6, Batch 90/600, Loss: 0.0594\n",
      "Epoch 3/6, Batch 100/600, Loss: 0.0779\n",
      "Epoch 3/6, Batch 110/600, Loss: 0.0895\n",
      "Epoch 3/6, Batch 120/600, Loss: 0.1446\n",
      "Epoch 3/6, Batch 130/600, Loss: 0.1057\n",
      "Epoch 3/6, Batch 140/600, Loss: 0.1442\n",
      "Epoch 3/6, Batch 150/600, Loss: 0.0912\n",
      "Epoch 3/6, Batch 160/600, Loss: 0.0272\n",
      "Epoch 3/6, Batch 170/600, Loss: 0.1231\n",
      "Epoch 3/6, Batch 180/600, Loss: 0.0703\n",
      "Epoch 3/6, Batch 190/600, Loss: 0.1235\n",
      "Epoch 3/6, Batch 200/600, Loss: 0.0948\n",
      "Epoch 3/6, Batch 210/600, Loss: 0.0753\n",
      "Epoch 3/6, Batch 220/600, Loss: 0.1348\n",
      "Epoch 3/6, Batch 230/600, Loss: 0.0763\n",
      "Epoch 3/6, Batch 240/600, Loss: 0.0626\n",
      "Epoch 3/6, Batch 250/600, Loss: 0.1720\n",
      "Epoch 3/6, Batch 260/600, Loss: 0.0532\n",
      "Epoch 3/6, Batch 270/600, Loss: 0.0900\n",
      "Epoch 3/6, Batch 280/600, Loss: 0.0664\n",
      "Epoch 3/6, Batch 290/600, Loss: 0.0414\n",
      "Epoch 3/6, Batch 300/600, Loss: 0.0207\n",
      "Epoch 3/6, Batch 310/600, Loss: 0.0684\n",
      "Epoch 3/6, Batch 320/600, Loss: 0.0658\n",
      "Epoch 3/6, Batch 330/600, Loss: 0.1519\n",
      "Epoch 3/6, Batch 340/600, Loss: 0.0489\n",
      "Epoch 3/6, Batch 350/600, Loss: 0.0816\n",
      "Epoch 3/6, Batch 360/600, Loss: 0.1146\n",
      "Epoch 3/6, Batch 370/600, Loss: 0.0851\n",
      "Epoch 3/6, Batch 380/600, Loss: 0.2073\n",
      "Epoch 3/6, Batch 390/600, Loss: 0.1501\n",
      "Epoch 3/6, Batch 400/600, Loss: 0.0362\n",
      "Epoch 3/6, Batch 410/600, Loss: 0.0550\n",
      "Epoch 3/6, Batch 420/600, Loss: 0.0795\n",
      "Epoch 3/6, Batch 430/600, Loss: 0.0593\n",
      "Epoch 3/6, Batch 440/600, Loss: 0.1485\n",
      "Epoch 3/6, Batch 450/600, Loss: 0.0649\n",
      "Epoch 3/6, Batch 460/600, Loss: 0.1739\n",
      "Epoch 3/6, Batch 470/600, Loss: 0.0266\n",
      "Epoch 3/6, Batch 480/600, Loss: 0.1423\n",
      "Epoch 3/6, Batch 490/600, Loss: 0.0238\n",
      "Epoch 3/6, Batch 500/600, Loss: 0.0334\n",
      "Epoch 3/6, Batch 510/600, Loss: 0.0504\n",
      "Epoch 3/6, Batch 520/600, Loss: 0.1020\n",
      "Epoch 3/6, Batch 530/600, Loss: 0.0841\n",
      "Epoch 3/6, Batch 540/600, Loss: 0.1167\n",
      "Epoch 3/6, Batch 550/600, Loss: 0.0848\n",
      "Epoch 3/6, Batch 560/600, Loss: 0.1232\n",
      "Epoch 3/6, Batch 570/600, Loss: 0.1096\n",
      "Epoch 3/6, Batch 580/600, Loss: 0.0836\n",
      "Epoch 3/6, Batch 590/600, Loss: 0.0407\n",
      "Epoch 4/6, Batch 0/600, Loss: 0.0495\n",
      "Epoch 4/6, Batch 10/600, Loss: 0.0467\n",
      "Epoch 4/6, Batch 20/600, Loss: 0.0198\n",
      "Epoch 4/6, Batch 30/600, Loss: 0.1143\n",
      "Epoch 4/6, Batch 40/600, Loss: 0.1093\n",
      "Epoch 4/6, Batch 50/600, Loss: 0.2332\n",
      "Epoch 4/6, Batch 60/600, Loss: 0.0391\n",
      "Epoch 4/6, Batch 70/600, Loss: 0.0459\n",
      "Epoch 4/6, Batch 80/600, Loss: 0.0478\n",
      "Epoch 4/6, Batch 90/600, Loss: 0.0358\n",
      "Epoch 4/6, Batch 100/600, Loss: 0.0260\n",
      "Epoch 4/6, Batch 110/600, Loss: 0.0510\n",
      "Epoch 4/6, Batch 120/600, Loss: 0.0524\n",
      "Epoch 4/6, Batch 130/600, Loss: 0.0865\n",
      "Epoch 4/6, Batch 140/600, Loss: 0.0610\n",
      "Epoch 4/6, Batch 150/600, Loss: 0.0251\n",
      "Epoch 4/6, Batch 160/600, Loss: 0.0512\n",
      "Epoch 4/6, Batch 170/600, Loss: 0.0812\n",
      "Epoch 4/6, Batch 180/600, Loss: 0.0183\n",
      "Epoch 4/6, Batch 190/600, Loss: 0.0509\n",
      "Epoch 4/6, Batch 200/600, Loss: 0.0685\n",
      "Epoch 4/6, Batch 210/600, Loss: 0.0986\n",
      "Epoch 4/6, Batch 220/600, Loss: 0.0611\n",
      "Epoch 4/6, Batch 230/600, Loss: 0.0605\n",
      "Epoch 4/6, Batch 240/600, Loss: 0.1333\n",
      "Epoch 4/6, Batch 250/600, Loss: 0.1031\n",
      "Epoch 4/6, Batch 260/600, Loss: 0.0842\n",
      "Epoch 4/6, Batch 270/600, Loss: 0.0316\n",
      "Epoch 4/6, Batch 280/600, Loss: 0.0884\n",
      "Epoch 4/6, Batch 290/600, Loss: 0.0450\n",
      "Epoch 4/6, Batch 300/600, Loss: 0.0472\n",
      "Epoch 4/6, Batch 310/600, Loss: 0.0690\n",
      "Epoch 4/6, Batch 320/600, Loss: 0.0677\n",
      "Epoch 4/6, Batch 330/600, Loss: 0.0567\n",
      "Epoch 4/6, Batch 340/600, Loss: 0.0375\n",
      "Epoch 4/6, Batch 350/600, Loss: 0.0116\n",
      "Epoch 4/6, Batch 360/600, Loss: 0.0582\n",
      "Epoch 4/6, Batch 370/600, Loss: 0.1261\n",
      "Epoch 4/6, Batch 380/600, Loss: 0.0617\n",
      "Epoch 4/6, Batch 390/600, Loss: 0.0553\n",
      "Epoch 4/6, Batch 400/600, Loss: 0.1238\n",
      "Epoch 4/6, Batch 410/600, Loss: 0.1026\n",
      "Epoch 4/6, Batch 420/600, Loss: 0.2368\n",
      "Epoch 4/6, Batch 430/600, Loss: 0.0713\n",
      "Epoch 4/6, Batch 440/600, Loss: 0.0667\n",
      "Epoch 4/6, Batch 450/600, Loss: 0.1000\n",
      "Epoch 4/6, Batch 460/600, Loss: 0.1080\n",
      "Epoch 4/6, Batch 470/600, Loss: 0.0746\n",
      "Epoch 4/6, Batch 480/600, Loss: 0.0311\n",
      "Epoch 4/6, Batch 490/600, Loss: 0.0502\n",
      "Epoch 4/6, Batch 500/600, Loss: 0.1037\n",
      "Epoch 4/6, Batch 510/600, Loss: 0.1198\n",
      "Epoch 4/6, Batch 520/600, Loss: 0.1009\n",
      "Epoch 4/6, Batch 530/600, Loss: 0.1519\n",
      "Epoch 4/6, Batch 540/600, Loss: 0.0649\n",
      "Epoch 4/6, Batch 550/600, Loss: 0.0675\n",
      "Epoch 4/6, Batch 560/600, Loss: 0.0141\n",
      "Epoch 4/6, Batch 570/600, Loss: 0.0912\n",
      "Epoch 4/6, Batch 580/600, Loss: 0.0504\n",
      "Epoch 4/6, Batch 590/600, Loss: 0.0917\n",
      "Epoch 5/6, Batch 0/600, Loss: 0.0954\n",
      "Epoch 5/6, Batch 10/600, Loss: 0.0390\n",
      "Epoch 5/6, Batch 20/600, Loss: 0.0215\n",
      "Epoch 5/6, Batch 30/600, Loss: 0.0325\n",
      "Epoch 5/6, Batch 40/600, Loss: 0.0154\n",
      "Epoch 5/6, Batch 50/600, Loss: 0.0511\n",
      "Epoch 5/6, Batch 60/600, Loss: 0.0338\n",
      "Epoch 5/6, Batch 70/600, Loss: 0.0843\n",
      "Epoch 5/6, Batch 80/600, Loss: 0.0439\n",
      "Epoch 5/6, Batch 90/600, Loss: 0.0558\n",
      "Epoch 5/6, Batch 100/600, Loss: 0.0212\n",
      "Epoch 5/6, Batch 110/600, Loss: 0.0668\n",
      "Epoch 5/6, Batch 120/600, Loss: 0.0188\n",
      "Epoch 5/6, Batch 130/600, Loss: 0.1855\n",
      "Epoch 5/6, Batch 140/600, Loss: 0.0240\n",
      "Epoch 5/6, Batch 150/600, Loss: 0.0776\n",
      "Epoch 5/6, Batch 160/600, Loss: 0.0139\n",
      "Epoch 5/6, Batch 170/600, Loss: 0.0541\n",
      "Epoch 5/6, Batch 180/600, Loss: 0.1013\n",
      "Epoch 5/6, Batch 190/600, Loss: 0.0412\n",
      "Epoch 5/6, Batch 200/600, Loss: 0.0255\n",
      "Epoch 5/6, Batch 210/600, Loss: 0.0918\n",
      "Epoch 5/6, Batch 220/600, Loss: 0.0179\n",
      "Epoch 5/6, Batch 230/600, Loss: 0.0641\n",
      "Epoch 5/6, Batch 240/600, Loss: 0.1007\n",
      "Epoch 5/6, Batch 250/600, Loss: 0.0497\n",
      "Epoch 5/6, Batch 260/600, Loss: 0.0418\n",
      "Epoch 5/6, Batch 270/600, Loss: 0.0517\n",
      "Epoch 5/6, Batch 280/600, Loss: 0.0722\n",
      "Epoch 5/6, Batch 290/600, Loss: 0.1519\n",
      "Epoch 5/6, Batch 300/600, Loss: 0.0252\n",
      "Epoch 5/6, Batch 310/600, Loss: 0.0906\n",
      "Epoch 5/6, Batch 320/600, Loss: 0.0264\n",
      "Epoch 5/6, Batch 330/600, Loss: 0.0355\n",
      "Epoch 5/6, Batch 340/600, Loss: 0.0657\n",
      "Epoch 5/6, Batch 350/600, Loss: 0.0446\n",
      "Epoch 5/6, Batch 360/600, Loss: 0.0706\n",
      "Epoch 5/6, Batch 370/600, Loss: 0.0594\n",
      "Epoch 5/6, Batch 380/600, Loss: 0.0221\n",
      "Epoch 5/6, Batch 390/600, Loss: 0.0701\n",
      "Epoch 5/6, Batch 400/600, Loss: 0.0485\n",
      "Epoch 5/6, Batch 410/600, Loss: 0.0647\n",
      "Epoch 5/6, Batch 420/600, Loss: 0.0520\n",
      "Epoch 5/6, Batch 430/600, Loss: 0.0188\n",
      "Epoch 5/6, Batch 440/600, Loss: 0.1780\n",
      "Epoch 5/6, Batch 450/600, Loss: 0.0463\n",
      "Epoch 5/6, Batch 460/600, Loss: 0.0521\n",
      "Epoch 5/6, Batch 470/600, Loss: 0.0272\n",
      "Epoch 5/6, Batch 480/600, Loss: 0.0620\n",
      "Epoch 5/6, Batch 490/600, Loss: 0.0912\n",
      "Epoch 5/6, Batch 500/600, Loss: 0.0330\n",
      "Epoch 5/6, Batch 510/600, Loss: 0.1102\n",
      "Epoch 5/6, Batch 520/600, Loss: 0.0374\n",
      "Epoch 5/6, Batch 530/600, Loss: 0.0265\n",
      "Epoch 5/6, Batch 540/600, Loss: 0.0131\n",
      "Epoch 5/6, Batch 550/600, Loss: 0.0241\n",
      "Epoch 5/6, Batch 560/600, Loss: 0.0845\n",
      "Epoch 5/6, Batch 570/600, Loss: 0.0879\n",
      "Epoch 5/6, Batch 580/600, Loss: 0.0673\n",
      "Epoch 5/6, Batch 590/600, Loss: 0.0328\n",
      "Epoch 6/6, Batch 0/600, Loss: 0.0358\n",
      "Epoch 6/6, Batch 10/600, Loss: 0.0700\n",
      "Epoch 6/6, Batch 20/600, Loss: 0.0212\n",
      "Epoch 6/6, Batch 30/600, Loss: 0.1034\n",
      "Epoch 6/6, Batch 40/600, Loss: 0.0351\n",
      "Epoch 6/6, Batch 50/600, Loss: 0.0414\n",
      "Epoch 6/6, Batch 60/600, Loss: 0.0776\n",
      "Epoch 6/6, Batch 70/600, Loss: 0.0203\n",
      "Epoch 6/6, Batch 80/600, Loss: 0.0319\n",
      "Epoch 6/6, Batch 90/600, Loss: 0.0271\n",
      "Epoch 6/6, Batch 100/600, Loss: 0.0428\n",
      "Epoch 6/6, Batch 110/600, Loss: 0.0230\n",
      "Epoch 6/6, Batch 120/600, Loss: 0.0315\n",
      "Epoch 6/6, Batch 130/600, Loss: 0.0278\n",
      "Epoch 6/6, Batch 140/600, Loss: 0.0777\n",
      "Epoch 6/6, Batch 150/600, Loss: 0.0431\n",
      "Epoch 6/6, Batch 160/600, Loss: 0.0396\n",
      "Epoch 6/6, Batch 170/600, Loss: 0.0311\n",
      "Epoch 6/6, Batch 180/600, Loss: 0.0408\n",
      "Epoch 6/6, Batch 190/600, Loss: 0.0130\n",
      "Epoch 6/6, Batch 200/600, Loss: 0.0142\n",
      "Epoch 6/6, Batch 210/600, Loss: 0.0256\n",
      "Epoch 6/6, Batch 220/600, Loss: 0.0252\n",
      "Epoch 6/6, Batch 230/600, Loss: 0.0295\n",
      "Epoch 6/6, Batch 240/600, Loss: 0.0152\n",
      "Epoch 6/6, Batch 250/600, Loss: 0.0367\n",
      "Epoch 6/6, Batch 260/600, Loss: 0.0253\n",
      "Epoch 6/6, Batch 270/600, Loss: 0.0261\n",
      "Epoch 6/6, Batch 280/600, Loss: 0.0075\n",
      "Epoch 6/6, Batch 290/600, Loss: 0.0103\n",
      "Epoch 6/6, Batch 300/600, Loss: 0.0228\n",
      "Epoch 6/6, Batch 310/600, Loss: 0.0176\n",
      "Epoch 6/6, Batch 320/600, Loss: 0.0202\n",
      "Epoch 6/6, Batch 330/600, Loss: 0.0311\n",
      "Epoch 6/6, Batch 340/600, Loss: 0.0246\n",
      "Epoch 6/6, Batch 350/600, Loss: 0.0716\n",
      "Epoch 6/6, Batch 360/600, Loss: 0.0207\n",
      "Epoch 6/6, Batch 370/600, Loss: 0.0247\n",
      "Epoch 6/6, Batch 380/600, Loss: 0.0517\n",
      "Epoch 6/6, Batch 390/600, Loss: 0.0137\n",
      "Epoch 6/6, Batch 400/600, Loss: 0.0238\n",
      "Epoch 6/6, Batch 410/600, Loss: 0.0090\n",
      "Epoch 6/6, Batch 420/600, Loss: 0.0611\n",
      "Epoch 6/6, Batch 430/600, Loss: 0.2169\n",
      "Epoch 6/6, Batch 440/600, Loss: 0.0201\n",
      "Epoch 6/6, Batch 450/600, Loss: 0.0230\n",
      "Epoch 6/6, Batch 460/600, Loss: 0.0411\n",
      "Epoch 6/6, Batch 470/600, Loss: 0.0673\n",
      "Epoch 6/6, Batch 480/600, Loss: 0.0754\n",
      "Epoch 6/6, Batch 490/600, Loss: 0.0385\n",
      "Epoch 6/6, Batch 500/600, Loss: 0.0234\n",
      "Epoch 6/6, Batch 510/600, Loss: 0.0283\n",
      "Epoch 6/6, Batch 520/600, Loss: 0.0212\n",
      "Epoch 6/6, Batch 530/600, Loss: 0.1136\n",
      "Epoch 6/6, Batch 540/600, Loss: 0.0137\n",
      "Epoch 6/6, Batch 550/600, Loss: 0.0231\n",
      "Epoch 6/6, Batch 560/600, Loss: 0.0266\n",
      "Epoch 6/6, Batch 570/600, Loss: 0.0322\n",
      "Epoch 6/6, Batch 580/600, Loss: 0.0617\n",
      "Epoch 6/6, Batch 590/600, Loss: 0.0277\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5eb16610-b1ed-43e0-bc53-df08fca81ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 97.60%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "print(f\"Accuracy on test set: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bba78a13-d6a9-4513-a936-167feee7d45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")#deploying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b59bfebc-d47c-417d-9394-05f536b50767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted digit: 1\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image \n",
    "\n",
    "image_path = '4.png' # Replace 'image.png' with your image file\n",
    "image = Image.open(image_path).convert('L') # Convert to grayscale\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)), # Resize to MNIST size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) # Normalize the image\n",
    "])\n",
    "image = transform(image).unsqueeze(0) # Add a batch dimension\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    output = model(image)\n",
    "    _, predicted = torch.max(output.data, 1)\n",
    "print('Predicted digit:', predicted.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8a9901c6-8557-491e-bd02-fa954535bbdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model = NN() # Replace \\'mnist_model.pt\\' with your model file\\nmodel.load_state_dict(torch.load(\"model.pth\"))\\nmodel.eval()\\n\\n# Load and preprocess the image\\nimage_path = \\'4.png\\' # Replace \\'image.png\\' with your image file\\nimage = Image.open(image_path).convert(\\'L\\') # Convert to grayscale\\ntransform = transforms.Compose([\\n    transforms.Resize((28, 28)), # Resize to MNIST size\\n    transforms.ToTensor(),\\n    transforms.Normalize((0.1307,), (0.3081,)) # Normalize the image\\n])\\nimage = transform(image).unsqueeze(0) # Add a batch dimension\\n\\n# Make predictions\\nwith torch.no_grad():\\n    output = model(image)\\n    _, predicted = torch.max(output.data, 1)\\nprint(\\'Predicted digit:\\', predicted.item())'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"model = NN() # Replace 'mnist_model.pt' with your model file\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Load and preprocess the image\n",
    "image_path = '4.png' # Replace 'image.png' with your image file\n",
    "image = Image.open(image_path).convert('L') # Convert to grayscale\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)), # Resize to MNIST size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) # Normalize the image\n",
    "])\n",
    "image = transform(image).unsqueeze(0) # Add a batch dimension\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    output = model(image)\n",
    "    _, predicted = torch.max(output.data, 1)\n",
    "print('Predicted digit:', predicted.item())\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b45414d-385c-4372-9db9-049f08c104c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
